## Discussion {.page_break_before}

We proposed a semiautomatic computational workflow to evaluate FAIR maturity indicators for scientific data repositories in the life sciences.
We tested our method on two real use cases where researchers looked for datasets to answer their scientific questions.
The two cases scored similarly.
Finally, we created a FAIR balloon plot to summarize and compare our results, and we made our workflow open and reproducible.

To assess data FAIRness, we implemented criteria that
follow principles and guidelines recommended by the MIAG [@url:https://github.com/FAIRMetrics/Metrics/tree/master/MaturityIndicators/Gen2],
reuse concepts from similar studies in the literature [@tag:tag_dunning],[@tag:tag_weber], and
add new considerations (Table <a href="#maturity_indicators">2</a>).
As recommended by the MIAG guidelines, we implemented a computational approach,
although we opted for a different prospective.
In their guidelines, the MIAG suggests to calculate maturity indicators starting from a global unique identifier (GUID) (e.g. Inchi, DOI, Handle, URL).
However, a priori knowledge of a GUID often signifies that a researcher has already found and accessed the dataset s/he is going to reuse.
In addition, it assumes that the repository of interest provides unique identifiers, which is not the case for ArrayExpress and Gene Expression Omnibus, based on the information we retrieved from re3data.org.
Similarly to Weber et al. [@tag:tag_weber], we decided to start our computations from dataset retrieval.
We asked two researchers in our departments to show us how they looked for the datasets of interest and which keywords they used.
Then, we computationally reproduced their manual search by programmatically retrieving data and metadata using their same keywords.
We recognize that this approach limits the generalization of FAIRness calculation.
On one side, creating a use case for every dataset is extremely demanding,
on the other side, the same dataset could be used to answer different research questions.
However, we think that an exhaustive set of real use cases could provide valuable insights on how to practically achieve data FAIRness.


For each group of principles, FAIRness criteria are:

- *Findability*:
The criteria to assess principles F1 (unique identifier), F3 (metadata includes identifier), and F4 ((meta)data are indexed) are similar for all authors.
In our case, to assess F1 we investigated whether a repository provides DOI in the registry re3data.org.
We chose this registry because it is one of the largest registry of scientific repositories, and it provides an open API.
For F3, we accepted any dataset identifier provided by the repository, as the principle does not explicitly mentions restrictions on the characteristics of the identifier.
Finally, for F4 we looked for dataset titles in Google Dataset Search.
We chose searchable resource because it could become one of the main search engines specific for data in the future, similarly to Google Scholar for publications.
Differently from the previous maturity indicators, the implementation of F2 (data are described with rich metadata) has large variations across authors.
The MIAG recommends to evaluate whether metadata contains "structured" elements,
Dunning et al. looked for attributes that favor findability, whereas
Weber et al. used metrics of time and space.
We followed the criteria suggested by Dunning et al. and looked for the keywords that researchers had used in their manual search to *find* datasets.

- *Accessibility*:
Similarly to the other authors, we retrieved our data using the HTTP protocol, which is free, open and allows for authentication, and thus satisfies all the requirements of the A1 group.
Also, there is concordance among authors for the principle A2, which requires that a repository should explicitly provide a policy for data availability.
In our implementation, we looked for the policy in re3data.org.

- *Interoperable*:
Similarly to the MIAG, we assigned a positive score to metadata in a structured file format, such as `xml` (I1).
On the other side, Dunning et al. and Weber et al. suggested that metadata should be in a standardized schema, such as [Dublin Core](https://www.dublincore.org/) or [DataCite](https://schema.datacite.org/), which would increase data interoperability and make retrieval easier.
None of the studies assessed I2 (vocabularies are FAIR), because it would require a separate implementation that includes the recursive nature of the FAIR principles.
Finally, for I3 all authors looked for references to other dataset in metadata.

- *Reusable*:
Although, the MIAG does not provide any guideline, authors implemented different ways to assess R1 (plurality of relevant attributes).
While Weber et al. used the same metrics as for F2, Dunning et al. focused on metadata that provide information on how to reuse a dataset.
In our implementation, we assess the presence of metadata attributes other than search keywords.
The principles R11 (availability of data usage license) and R12 (data provenance) had a straight-forward implementation for all authors.
In our approach, we looked for a data license in re3data.org and for authors, author emails, and title of the corresponding publication in the metadata from the dataset repository.
Finally, none of the authors evaluated whether metadata follow community standards (R13), as community agreements are not formally established yet.

We assessed FAIR maturity indicators using a mixed manual and automatic approach.
In the literature, Dunning at al. used a fully manual approach to assess the maturity indicators, whereas
Weber et al. used a completely automatic approach, calculating 10 of the total 15 maturity indicators.
Our mixed approach allowed us to automatically assess maturity indicators wherever possible, and to manually complement when we could not retrieve information via API.  
Because repositories do not use a standardized metadata schema, our mixed implementation required prior manual investigation of metadata attributes for each repository.
For example, ArrayExpress uses the attributes "authors", "email", and "title" that we could use for the principle R12, whereas Gene Express Omnibus does not have attributes for provenance.

To summarize and compare dataset FAIRness, we created a FAIR balloon plot.
As the MIAG guidelines recommend, we decided not to create a final score to avoid concerns for data and resource providers [@tag:tag_wilkinson_mi].
In our plot, we combined colors, sizes, and shapes of graphical elements to provide a summary of principles, scores, and type of information retrieval (manual, automatic, not assessed) for each dataset.
In this visualization, a dataset that reached full FAIRness would have all maturity indicators depicted as circles with maximum size, meaning full score and automatic retrieval.
In addition, by vertically stacking representations for different datasets, we can visually compare FAIRness levels for each maturity indicator.
In the literature, another example of visualization are *insigna*, created for the platform FAIRshake [@doi:10.1101/657676].
They consist of multiple squares colored from blue (satisfactory) to red (unsatisfactory) for different levels of FAIRness.
In addition, they can dynamically expand to visualize multiple scores calculated using different rubrics (i.e. criteria).
Although this representation embeds the possibility of using different criteria, it does not allow direct comparison across datasets.
Finally, we applied our FAIR balloon plot to the results collected by Dunning et al.  to demonstrate that this kind of visualization can be reused for FAIR assessment with other criteria (Figure @fig:dunning).  

To make our analysis open and reproducible, we implemented our workflow in a Jupyter notebook.
However, changes of APIs or metadata attributes could affect reproducibility of our work.
The possibility of querying a specific version of a repository would allow full reproducibility of workflows like ours.
Finally, we implemented our approach in python, a language increasingly used in various scientific communities that can potentially favor extension and reuse of our work.

The two datasets we analyzed (*Parkinsons_AE* and *NBIA_GEO*) met the majority of the criteria we defined to assess FAIRness.
Higher FAIRness compliance could be reached by using a standard schema for metadata (e.g. [Dublin Core](https://www.dublincore.org/), [DataCite](https://schema.datacite.org/), or [schema.org](www.schema.org), which could include all attributes required by the principles, and by
providing explicit information about data policy, licenses, etc. to registries of repositories.

In conclusion, we proposed a reproducible computational workflow to assess data FAIRness in the life sciences, and we created a FAIR balloon plot to summarize and compare FAIRness compliance.
We evaluated our approach on two real cases, and we demonstrated that the FAIR balloon plot can be extended to other FAIRness analyses.
Finally, we suggested that use of standard schema for metadata and presence of specific attributes in registries of repositories could increase FAIRness of datasets.



<!--Figure: dunning-->
![FAIR balloon plot for the repositories analyzed by Dunning et al. [@tag:tag_dunning] (data available at their [institutional repository](https://data.4tu.nl/repository/uuid:5146dd06-98e4-426c-9ae5-dc8fa65c549f)). From their quantitative scores, we converted "complies completely" to 1, "just about/maybe not" to 0.5, and "fails to comply" to 0. We did not assign any value to "unclear", which is thus represented as missing elements.](images/dunning.svg){#fig:dunning width="65%"}
