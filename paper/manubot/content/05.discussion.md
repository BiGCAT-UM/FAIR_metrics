## Discussion {.page_break_before}

We proposed a semiautomatic computational workflow to evaluate FAIR maturity indicators for scientific data repositories in the life sciences.
We tested our method on two real use cases where researchers looked for datasets to answer their scientific questions.
The two cases scored similarly and we compared them through a visualization.  

To assess data FAIRness, we implemented criteria that
follow principles and guidelines recommended by the MIAG [@url:https://github.com/FAIRMetrics/Metrics/tree/master/MaturityIndicators/Gen2],
reuse concepts from similar studies in the literature [@tag:tag_dunning] and [@tag:tag_weber], and
add new considerations (Table <a href="#maturity_indicators">2</a>).
As recommended by the MIAG guidelines, we implemented a computational approach,
although we opted for a different prospective.
In their guidelines, the MIAG suggests to calculate maturity indicators starting from a global unique identifier (GUID) (e.g. Inchi, DOI, Handle, URL).
However, a priori knowledge of a GUID often signifies that a researcher has already found and accessed the dataset s/he is going to reuse.
In addition, it assumes that the repository of interest provides unique identifiers, which is not the case for ArrayExpress and Gene Expression Omnibus, based on the information we retrieved from re3data.
To overcome these limitations, we decided to start our computations from dataset retrieval.
We asked two researchers in our departments to show us how they looked for the datasets of interest and which keywords they used.
Then, we computationally reproduced their manual search by programmatically retrieving data and metadata using their same keywords.
Our approach is similar to the one implemented by Weber et al., as they focused on a specific use case, and differs from the method used by Dunning et al., who analyzed repositories.

<!--From the manual approach presented by Dunning et al [cit] we reused some valuable practical criteria to assess data FAIRness.
First, we embraced their definitions for the criteria F2 (data are described with rich metadata) and R1 ((meta)data are richly described with a plurality of attributes).
In their approach, they defined the metadata as F2 as attributes that can "help findability", whereas R1 are attributes that "help one evaluate how reusable a dataset it".
We translated this to keywords and all the other attributes contained in the metadata.
From the approach by Dunning et al. [cit] we took the difference between F2 and R1, amd Title for F4 criteria R12
Finally, from weber et al. we took the computational aspect and the specific of one cases-->

The criteria we used were:


- *Findability*:
The criteria to assess principles F1 (unique identifier), F3 (metadata includes identifier), and F4 ((meta)data are indexed) are similar in guidelines and studies in the literature.
To assess F1, we investigated whether a repository provides DOI in re3data for consistency across repositories.
For F3, we accepted identifiers that are not persistent.
Finally, for F4 we propose to look for dataset titles in Google Dataset Search as it could become one of the main search engines for data in the future.
The implementation of F2 (data are described with rich metadata) has large variations across authors.
The MIAG recommends to evaluate whether metadata contains "structured" elements,
Dunning et al. looked for attributes that favor findability, whereas
Weber et al. used metrics of time and space.
We followed the criteria suggested by Dunning et al. and we focused on the presence of the keywords that researchers had used in their manual search to *find* datasets.

- *Accessibility*:
Similarly to the other authors, we retrieved our data using the HTTP protocol, which is free, open and allows for authentication, and thus satisfies the requirements of the A1 group.
Also, there is concordance among authors for principle A2, which requires that a repository should explicitly provide a policy for data availability.

- *Interoperable*
Similarly to the MIAG, we assigned a positive score to metadata in a structured file format, such as `xml` (I1). On the other side, Dunning et al. and Weber et al. suggest that metadata should be in a standardized schema, such as [Dublin Core](https://www.dublincore.org/) or [DataCite](https://schema.datacite.org/), which would increase data interoperability and make retrieval easier.
None of the studies has assessed I2 (vocabularies are FAIR) yes, because it requires a separate specific implementation that includes the recursive nature of the FAIR principles. In addition, I2 represents a challenging and ambitious aspiration can be accomplished subsequently.
Finally, for I3 all authors looked for references to other dataset in metadata.

- *Reusable*
Although, the MIAG does not provide any guideline, authors implemented different ways to assess R1 (plurality of relevant attributes).
While Weber et al. used the same metrics as for F2, Dunning et al. focused on metadata that provide information on how to reuse a dataset.
In this implementation, we assess the presence of metadata other than search keywords.
The principles R11 (availability of data usage license) and R12 (data provenance) had a straight-forward implementation for all authors.
Finally, none of the authors have evaluated whether metadata follow community standards (R13), most likely because community agreements are not formally established.

In our computation, we assessed FAIR maturity indicators using a mixed manual and automatic approach.
In the literature, Dunning at al. used a manual approach, whereas
Weber et al. used fully automatic approach to calculate 10 of the total 15 maturity indicators.
Our mixed approach allowed us to assess automatically the maturity indicators that were easily retrievable, and to complement manually with the ones that were not retrievable via API.  hoping that in the future it will be possible to retrieve them automatically
Both our manual and automatic implementations required knowledge of data schema.
Specifically, we had to know in advance some keywords, such as "author", "email"
Similarly, we had to know the fields in re3data, such as "data availability policy" for A2 "datalicensename" and "datalicenseurl" for R1.1
In addition, we had to implement a manual change
For example, when assessing criteria F2 (keywords are in metadata), we had to change the keyword "true" that we used for data retrieval into "rawdata"


- We chose python as it is a language that is used in various scientific communities and thus could potentially provide extension and reuse of our work.
- We chose to use Jupyter notebooks for reproducibility of our results. However, databases change but they do not provide versions. Therefore, we can just declare the time stamps when our query was done. In addition, Jupyter notebooks are both machine and human readable, and easier to export to other domains that do not use specifically programming languages designed for the web  
To be fully reproducible, ideally repositories and registries should have a version of their database or provide a doi of metadata

<!-- Discussion of results and -->

<!--**Visualizing the FAIR maturity indicators**-->
To summarize and compare the FAIRness
 evaluation we used a visualization that embeds principles, scores, and type of information retrieval (manual, automatic, not assessed).
We created a visualization plot to summarize FAIR evaluation and compare results of various datasets.
 and implementations.
We chose not to create a final score in accordance (to uniform) with the recommendations for the FAIR guidelines that want to keep suggestions and not to assign a score.
The summary of metrics is provided by the fact the we exploited shapes, colors, and sizes to put all possible information.
On the other side, the fact that each row represent a dataset allows for comparison among datasets
A visual approach
Visualizing results for summary and comparison is an approach taken also by FAIRshake.
They created *insignas*,
which FAIRness using a color gradient from blue (satisfactory) to red (unsatisfactory).
The platform FAIRshake provide visualizations that can dinamically expand to fit scores calculated using different metrics.
Differently, we chose a static approach because these the FAIRshake insignias do not allow for comparison
has implemented visualizations called "insignas", where they use color gradents
- We chose to plot our results instead of providing a final score to avoid negative connotations (see FAIR metrics vs. maturity indicators).
  However, we wanted to be able to compare our results, so we used balloon plots, usually used for categorical data visualization and comparison. (FAIR shake uses visualizations too but they are not comparable)  
- NO final score because

![comparison](images/dunning.svg){#fig:dunning width="100%"}




Some limitations must be acknowledged.
First, our approach requires an a-priori knowledge of metadata structure.

Different repositories use different data structure.
Automatization occurs after a lot of manual investigation

- We had to adapt the code based on  API type and response schema.
  Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases  
- We considered use cases where all queries provided one final dataset. In real practice, researchers often need to compare subset of retrieved datasets manually because there are not enough information to discriminate them computationally (the information is present, but not machine-readable)  
Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases.  
Different repositories use different html tags to define their
- Database APIs do not allow to retrieve all the information that the user interface allows (example 1: ChEBI does not allow to retrieve information about reactions; example 2: Array Express has some metadata in tables that must be downloaded locally before being queried  

*Limitations of current implementation*
- Limitations: A1, A11, A12: retrieve information only via HTTP, so they are all true. If data is on a local excel file or database, then we need a different implementation
+ Retrieval is not via identifier but keywords
- Repositories not databases

*General considerations*
All the information needed are retrieved from metadata, not from data.
We did not investigate databases, but only repositories

In conclusion, we have proposed a computational implementation to calculate FAIR maturity indicators in the life sciences.
Similarities and differences with the other criteria
Conclusion:
- it would be great if all repos had similar schema
- it would be great if we could access everything via API



The main limitation of our approach is that it requires knowing a specific research question and thus it is hardly generalizable to a large number of cases.
In addition, the same dataset could be found for different research questions, and thus with a different set of keywords.
However, ...


We extracted information about the repositories from re3data. These information were about DOI, availability policy of metadata when data are not available, and data license.
We selected only one registry (re3data) and one search engine(Google Dataset search). In the first case, alternative can be FAIRshare, however it still does not provide an open API. on the other side, we could have choose a generic search engine, like google, but we considered pertinent to look for a dataset specifically in a dataset search engine.
We chose re3data as it provides an API for queries. Other registries, such as FAIRshare, do not provide open API yet.
Similarly, Google Dataset Search does not provide any API, so we did a manual search.

Both dataset top scored in the A1 and sub-principles, however NBIA_GEO did not have an automatically retrievable policy from re3data (which does not imply that the policy is automatically or manually retrievable somewhere.


  <!--
  The interpretation of the FAIR principles was not always trivial. Given that they are vague by definition, we had to take decision. We based our decisions on the principles themselves,
  the interpretation by the authors of the metrics and two previous studies that computed FAIR maturity indicators on a large
  -->
