## Discussion {.page_break_before}

**Heavily work in progress**


In this study, we created an approach to calculated FAIR maturity indicators in the life science.
We tested our approach on two use cases that are actual research questions asked in our department.


Discussion of the four groups.





We present a workflow to compute FAIR maturity indicators for data repositories in the life sciences.

for two use-cases of dataset retrieval in the life sciences.

**Discussion of results**



Comparison with other papers - create graph for dunning?
- Comments on the findings  

**How we calculated the metrics**
*Criteria*
We developed criteria to assess FAIRness based on the guidelines proposed by the group and from the practical implementations by Dunning and Weber.

We developed a mixed manual and automatic appraoach to assess FAIR metrics.
Dunning at al. used a manual approach, while Weber et al. used a fully computational approach.
We opted for a mixed approach because the guidelines stress on the importance of the fact that information should be retrieved by machines, therefore a complete manual approach is not desirable
On the other side, a fully computational approach is currently not possible because not all information are retrievable via API and in fact, Weber et al. calculated 2/3 of metrics.
Specifically, we ret
We used a mixed automatic and manual implementation to retrieve all the possible information with the hope that information now retrieved manually will in the close future be possible to retrieve them automatically.
Both the manual and the automatic retrieval of information required a large amount of time and the knowledge of data structure and schema.
Querying data repository required knowledge of metadata structure and of data field (i.e. xml tags).
Specifically, we had to know in advance some keywords, such as "author", "email"
Similary, we had to know the fields in re3data, such as "data availability policy" for A2 "datalicensename" and "datalicenseurl" for R1.1
In addition, we had to implement a manual change
For example, when assessing criteria F2 (keywords are in metadata), we had to change the keyword "true" that we used for data retrieval into "rawdata"
To simplyfy our computations, we put all tags lower case and vectorized for quick search.

We also simulated data retrieval. This is different from Wilkison because their metrics start with a globally unique identifier (GUID). However, when researchers look for data they do not know already the identifiers, they do not simulate what researchers do. In addition, this workflow would preclude the analysis of our two repositories as they do not provide GUIDs (F1).  

We extracted information about the repositories from re3data. These information were about DOI, availability policy of metadata when data are not available, and data license.
We selected only one registry (re3data) and one search engine(Google Dataset search). In the first case, alternative can be FAIRshare, however it still does not provide an open API. on the other side, we could have choose a generic search engine, like google, but we considered pertinent to look for a dataset specifically in a dataset search engine.
To compute the metrics we follow

Similarly to Dunning et al. and to Weber et al. we did not computer two principles. The first isthe principle I2. i.e. presence of a fair metadata vocabulary. The difficulty raises from the fact that
The second is R13, which is that metadata follow community standards at those have not been formally established yet.

comparison with dunning: mainly the criteria expecially difference between metadata for findable and reusable.
comparison with Weber: re3data,
comparison with Wilkinson: assumes that the GUID is already know thus skipping the finding of the dataset - this is not compatible with our findable criteria

Similarity with all cases:


*Practical implementation*
Before calculating the metrics we decided to simulate dataset retrieval via API. We assumed that a researcher does not know a priori the. Sometimes you dont get only 1 datasets


*Difficulties with the three repositories*
To answer our criteria, we queried three different kinds of repositories: Data repositories, re3data and Google Dataset Search.


We chose re3data as it provides an API for queries. Other registries, such as FAIRshare, do not provide open API yet.

Similarly, Google Dataset Search does not provide any API, so we did a manual search.

Manual vs automatic retrieval


*Comparison to the other works*

- Comparison to Wilkinson: Our metrics do not start with metadata GUID (general user identifier) (see gen2) but with the researcher's question. Using GUID implies that the researcher has already found the dataset of interest  
- Compare to  Weber and Dunning  


*Limitations of current implementation*
- Limitations: A1, A11, A12: retrieve information only via HTTP, so they are all true. If data is on a local excel file or database, then we need a different implementation
- Repositories not databases




**Visualizing the FAIR maturity indicators**

We visualized t
This visualization allowed us to summarize FAIR evaluation and compare the results of various dataset and implementations.
We chose not to create a final score in accordance (to uniform) with the recommendations for the FAIR guidelines that want to keep suggestions and not to assign a score.
The summary of metrics is provided by the fact the we exploited shapes, colors, and sizes to put all possible information.
On the other side, the fact that each row represent a dataset allows for comparison among datasets
The platform FAIRshake provide visualizations that can dinamically expand to fit scores calculated using different metrics.
Scores are in a range that goes from blue (satisfactory) to red (unsatisfactory).
Differently, we chose a static approach because these the FAIRshake insignias do not allow for comparison
has implemented visualizations called "insignas", where they use color gradents

- We chose to plot our results instead of providing a final score to avoid negative connotations (see FAIR metrics vs. maturity indicators).
  However, we wanted to be able to compare our results, so we used balloon plots, usually used for categorical data visualization and comparison. (FAIR shake uses visualizations too but they are not comparable)  
- NO final score because

**Reproducibility**
- We chose to use Jupyter notebooks for reproducibility of our results. However, databases change but they do not provide versions. Therefore, we can just declare the time stamps when our query was done. In addition, Jupyter notebooks are both machine and human readable, and easier to export to other domains that do not use specifically programming languages designed for the web  

**Difficulty / Limitations**

Some limitations must be acknowledged.
First, our approach requires an a-priori knowledge of metadata structure.

Different repositories use different data structure.
Automatization occurs after a lot of manual investigation

- We had to adapt the code based on  API type and response schema.
  Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases  
- We considered use cases where all queries provided one final dataset. In real practice, researchers often need to compare subset of retrieved datasets manually because there are not enough information to discriminate them computationally (the information is present, but not machine-readable)  
Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases.  
Different repositories use different html tags to define their
- Database APIs do not allow to retrieve all the information that the user interface allows (example 1: ChEBI does not allow to retrieve information about reactions; example 2: Array Express has some metadata in tables that must be downloaded locally before being queried  


*General considerations*
All the information needed are retrieved from metadata, not from data.
We did not investigate databases, but only repositories

Conclusion:
- it would be great if all repos had similar schema
- it would be great if we could access everything via API

  <!--
  The interpretation of the FAIR principles was not always trivial. Given that they are vague by definition, we had to take decision. We based our decisions on the principles themselves,
  the interpretation by the authors of the metrics and two previous studies that computed FAIR maturity indicators on a large
  -->
