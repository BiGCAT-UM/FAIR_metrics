## Discussion {.page_break_before}

**This is still quite a lot work in progress, ready in 1-2 days**


We present a workflow to compute FAIR maturity indicators for data repositories in the life sciences.

for two use-cases of dataset retrieval in the life sciences.

**Discussion of results**
Comparison with other papers - create graph for dunning?
- Comments on the findings  

**How we calculated the metrics**
Before calculating the metrics we decided to simulate dataset retrieval via API. We assumed that a researcher does not know a priori the


*Difficulties with the three repositories*
To answer our criteria, we queried three different kinds of repositories: Data repositories, re3data and Google Dataset Search.

Querying data repository required knowledge of metadata structure and of data field (i.e. xml tags).
Specifically, we had to know in advance some keywords, such as "author", "email"

Similary, we had to know the fields in re3data, such as "data availability policy" for A2 "datalicensename" and "datalicenseurl" for R1.1
In addition, we had to implement a manual change
For example, when assessing criteria F2 (keywords are in metadata), we had to change the keyword "true" that we used for data retrieval into "rawdata"
To simplyfy our computations, we put all tags lower case and vectorized for quick search

We chose re3data as it provides an API for queries. Other registries, such as FAIRshare, do not provide open API yet.

Similarly, Google Dataset Search does not provide any API, so we did a manual search.



*Comparison to the other works*

- Comparison to Wilkinson: Our metrics do not start with metadata GUID (general user identifier) (see gen2) but with the researcher's question. Using GUID implies that the researcher has already found the dataset of interest  
- Compare to  Weber and Dunning  


*Limitations of current implementation*
- Limitations: A1, A11, A12: retrieve information only via HTTP, so they are all true. If data is on a local excel file or database, then we need a different implementation
- Repositories not databases




**Visualizing the FAIR maturity indicators**

We visualized t
This visualization allowed us to summarize FAIR evaluation and compare the results of various dataset and implementations.
We chose not to create a final score in accordance (to uniform) with the recommendations for the FAIR guidelines that want to keep suggestions and not to assign a score.
The summary of metrics is provided by the fact the we exploited shapes, colors, and sizes to put all possible information.
On the other side, the fact that each row represent a dataset allows for comparison among datasets
The platform FAIRshake provide visualizations that can dinamically expand to fit scores calculated using different metrics.
Scores are in a range that goes from blue (satisfactory) to red (unsatisfactory).
Differently, we chose a static approach because these the FAIRshake insignias do not allow for comparison
has implemented visualizations called "insignas", where they use color gradents

- We chose to plot our results instead of providing a final score to avoid negative connotations (see FAIR metrics vs. maturity indicators).
  However, we wanted to be able to compare our results, so we used balloon plots, usually used for categorical data visualization and comparison. (FAIR shake uses visualizations too but they are not comparable)  
- NO final score because

**Reproducibility**
- We chose to use Jupyter notebooks for reproducibility of our results. However, databases change but they do not provide versions. Therefore, we can just declare the time stamps when our query was done. In addition, Jupyter notebooks are both machine and human readable, and easier to export to other domains that do not use specifically programming languages designed for the web  

**Difficulty / Limitations**

Some limitations must be acknowledged.
First, our approach requires an a-priori knowledge of metadata structure.

Different repositories use different data structure.
Automatization occurs after a lot of manual investigation

- We had to adapt the code based on  API type and response schema.
  Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases  
- We considered use cases where all queries provided one final dataset. In real practice, researchers often need to compare subset of retrieved datasets manually because there are not enough information to discriminate them computationally (the information is present, but not machine-readable)  
Our implementation requires specific knowledge of the database structure and thus it is difficult to directly generalize it to various databases.  
Different repositories use different html tags to define their
- Database APIs do not allow to retrieve all the information that the user interface allows (example 1: ChEBI does not allow to retrieve information about reactions; example 2: Array Express has some metadata in tables that must be downloaded locally before being queried  


  <!--
  The interpretation of the FAIR principles was not always trivial. Given that they are vague by definition, we had to take decision. We based our decisions on the principles themselves,
  the interpretation by the authors of the metrics and two previous studies that computed FAIR maturity indicators on a large
  -->
