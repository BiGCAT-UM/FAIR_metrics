## Introduction {.page_break_before}


<!--DATA SHARING AND REUSE-->
Data sharing and data reuse are two complementary aspects of modern research activity.
Researchers share their data
for a sense of community,
to demonstrate integrity of acquired data,
and to enhance quality and reproducibility of research work [@doi:10.1371/journal.pone.0189288].
In addition, data sharing is supported by
the emerging citation system for datasets,
scientific journal requirements,
and funding agencies that want to maximize their return on investments in science [@doi:10.1038/d41586-019-01715-4], [@doi:10.1371/journal.pone.0018657].
At the same time, researchers are eager to reuse available data
to integrate information that answer interdisciplinary research questions
and to optimize use of fundings [@doi:10.1002/pra2.2018.14505501060].
Although attitudes towards data sharing and reuse are increasingly favorable [@doi:10.1371/journal.pone.0189288], data discovery and reuse remain difficult in practice [@doi:10.1007/s10502-014-9236-y].
Studies show that 40%  of  qualitative  datasets  were  never downloaded, and about 25% of data is used only up to 10 times [@doi:10.1177/2158244016685136].
In addition, Vines et al. demonstrated that data availability decreases 17% per year due to lack of appropriate hardware to access old storage media or because data were lost [@doi:10.1016/j.cub.2013.11.014].
To be effective, data sharing and reuse need appropriate infrastructure, standards, and policies [@doi:10.1007/s10502-014-9236-y].

To increase data reuse in the life sciences,
the [FORCE 11](https://www.force11.org/) group proposed guidelines in 2016.
These guidelines aimed to make data findable, accessible, interoperable, and reusable, and were summarized in the acronym FAIR [@tag:tag_wilkinson_fair].
In a short time the FAIR guidelines have gained remarkable popularity, and  they are currently supported by funding agencies and political entities, such as the European Commission, the National Institutes of Health in the United States, and institutions in Africa and Australia [@doi:10.3233/ISU-170824].
In addition, academic and institutional initiatives were launched to promote and implement data FAIRness, such as [GOFAIR](https://www.go-fair.org/) and [FAIRsharing](https://fairsharing.org/).
Although largely adopted, the FAIR principles do not specify any technical requirement as they are deliberately intended as aspirational [@doi:10.3233/ISU-170824].
The lack of practical specifications generated a large spectrum of interpretations and concerns and raised the need to define measurements of data FAIRness.
Some of the authors of the seminal paper proposed a set of FAIR metrics [@tag:tag_wilkinson_metrics], subsequently reformulated as FAIR maturity indicators [@tag:tag_wilkinson_mi].
At the same time, they invited consortia and communities to suggest and create alternative evaluators.
The majority of the proposed tools are online questionnaires that researchers and repository curators can manually fill to assess the FAIRness of their data (Table <a href="#literature">1</a>).
However, the FAIR metrics guidelines emphasize on the importance of creating "objective, quantitative, [and] machine-interpretable" evaluators [@tag:tag_wilkinson_metrics].
Following this criterion, two platforms have recently been developed to automatically compute FAIR maturity indicators: [FAIR Evaluation Services](https://fairsharing.github.io/FAIR-Evaluator-FrontEnd/#!/) and [FAIRshake](https://fairshake.cloud/).
The first platform offers evaluation of maturity indicators and compliance tests [@tag:tag_wilkinson_mi], whereas the second platform provides metrics, rubrics and evaluators for registered digital resources [@doi:10.1101/657676].
Both platforms provide use cases for FAIRness assessment, however they do not provide systematic analysis of evaluated datasets and repositories.
In the literature, two studies report evaluation of FAIRness for large datasets.
Dunning et al. [@tag:tag_dunning] used a qualitative approach to investigate 37 repositories and databases.
They assessed FAIRness using a traffic-light rating system that ranges from no to full compliance.
Differently, Weber et al. [@tag:tag_weber] implemented a computational workflow to analyze the retrieval of more than a million images from five repositories.
They proposed metrics specific for images, including time and place of acquisition to assess image provenance.
The first study provides valuable concrete guidelines to assess data FAIRness, however the implementation was manual, differently from what the guidelines suggest.
On the other side, the second study is a relevant example of computational implementation, although limited to retrieval of images and evaluation of 10 out of 15 criteria.

In this paper, we propose a computational approach to calculate FAIR maturity indicators in the life sciences.
We followed the recommendations provided by the Maturity Indicator Authoring Group (MIAG) [@tag:tag_wilkinson_mi] and we created a visualization tool to summarize and compare FAIR maturity indicators across various datasets and/or repositories.
We tested our approach on two real use cases where researchers retrieved data from scientific repositories to answer their research questions.
Finally, we made our work open and reproducible by implementing our computations in a Jupyter notebook using python.



<!-- Table1: FAIR principle evaluators-->
<a name="literature"></a>
<table style="width:100%;">
<caption>
<span>Table 1:</span>
Online FAIR evaluators and studies in the literature assessing FAIRness of data repositories (the symbol &#10003; indicates "yes", the symbol - indicates "no").
</caption>

<colgroup>
<col style="width: 13%" /> <!-- authors -->
<col style="width: 15%" /> <!-- tool -->
<col style="width: 10%" /> <!-- manual -->
<col style="width: 11%" /> <!-- automatic - code / language -->
<col style="width: 11%" /> <!-- automatic - metadata format -->
<col style="width: 11%" /> <!-- automatic - protocol  -->
<col style="width: 10%" /> <!-- repository -->
</colgroup>

<thead>
<tr class="header">
<th colspan="1">Authors</th>
<th colspan="1">Questionnaire / Platform</th>
<th colspan="1">Manual Assessment</th>
<th colspan="3">Automatic Assessment</th>
<th colspan="1">Data / Code Repository</th>
</tr>
</thead>

<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>Code / Language</td>
<td>Metadata Format</td>
<td>Protocol / Library</td>
<td></td>
</tr>

<thead>
<tr class="header">
<th colspan="7">FAIRness evaluators</th>
</tr>
</thead>

<!-- Wilkinsons - metrics -->
<tr class="even">
<td>Wilkinsons et al. [@tag:tag_wilkinson_metrics]</td>
<td>-</td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><a href="https://github.com/FAIRMetrics/Metrics/tree/master/MaturityIndicators/Gen1">GitHub</a></td>
</tr>

<!-- Australian Research Data Commons -->
<tr class="odd">
<td>Australian Research Data Commons</td>
<td><a href="https://www.ands-nectar-rds.org.au/fair-tool">FAIR self-assessment tool</a></td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>

<!-- Commonwealth Scientific and Industrial Research Organization -->
<tr class="even">
<td>Commonwealth Scientific and Industrial Research Organization</td>
<td><a href="http://oznome.csiro.au/5star/">5 star data rating tool</a></td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>

<!-- Data Archiving and Networked Services -->
<tr class="even">
<td>Data Archiving and Networked Services</td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSf7t1Z9IOBoj5GgWqik8KnhtH3B819Ch6lD5KuAz7yn0I0Opw/viewform">FAIR enough?</a> and <a href="https://www.surveymonkey.com/r/fairdat">FAIR data assessment tool</a></td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>

<!-- GOFAIR consortium -->
<tr class="odd">
<td>GOFAIR consortium</td>
<td><a href="https://docs.google.com/forms/d/1Oug6GowuG1jNZNsjklXOeEvPbUrhyuS_F-d185SOy6A/">FAIR ImplementationMatrix</a></td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><a href="https://osf.io/n7uwp/">Open Science Framework</a></td>
</tr>

<!-- EUDAT  -->
<tr class="odd">
<td>EUDAT2020</td>
<td><a href="https://www.edugroepen.nl/sites/RDM_platform/Shared%20Documents/Bij%20de%20WG%20Onderzoeksondersteuning%20en%20advies/How-FAIR-are-your-data.pdf">How FAIR are your data?</a>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><a href="https://www.doi.org/10.5281/zenodo.1065990">Zenodo</a></td>
</tr>


<!-- Wilkinsons - MI -->
<tr class="even">
<td>Wilkinsons et al. [@tag:tag_wilkinson_mi]</td>
<td><a href="https://fairsharing.github.io/FAIR-Evaluator-FrontEnd/#!/">FAIR evaluation services</a></td>
<td>-</td>
<td>Ruby on Rails</td>
<td>JSON, Microformat, JSSON-LD, RDFa</td>
<td>nanopublications</td>
<td><a href="https://github.com/FAIRMetrics/Metrics/tree/master/MaturityIndicators/Gen2">GitHub</a></td>
</tr>

<!-- Clark -->
<tr class="odd">
<td>Clark et al. [@doi:10.1101/657676]</td>
<td><a href="https://fairshake.cloud/">FAIRshake</a></td>
<td>-</td>
<td>Django and python</td>
<td>RDF</td>
<td>Extruct</td>
<td><a href="https://github.com/MaayanLab/FAIRshake">GitHub</a></td>
</tr>


<thead>
<tr class="header">
<th colspan="7">Studies assessing FAIRness of repositories</th>
</tr>
</thead>

<!-- Dunning -->
<tr class="even">
<td>Dunning et al. [@tag:tag_dunning] </td>
<td>-</td>
<td>&#10003;</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><a href="https://data.4tu.nl/repository/uuid:5146dd06-98e4-426c-9ae5-dc8fa65c549f">Institutional repository</a></td>
</tr>

<!-- Weber -->
<tr class="odd">
<td>Weber et al. [@tag:tag_weber] </td>
<td>-</td>
<td>-</td>
<td>python</td>
<td>DataCite</td>
<td>OAI-PMH</td>

<td><a href="https://gitlab.lrz.de/ubiquando/ubiquando">GitLab</a></td>
</tr>


<!-- Our approach -->
<tr class="odd">
<td>Our approach</td>
<td>-</td>
<td>&#10003; (partially)</td>
<td>Jupyter notebook with python</td>
<td>XML, JSON</td>
<td>request</td>

<td>GitHub</td>
</tr>

</tbody>
</table>
